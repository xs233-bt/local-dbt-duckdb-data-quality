<img width="448" height="200" alt="image" src="https://github.com/user-attachments/assets/182aa5e6-6849-42c7-b5e7-0a51ca38d496" />


# Local dbt + DuckDB Data Quality Pipeline

This project demonstrates a **local-first analytics and data quality workflow**
using **dbt** and **DuckDB**, designed to validate data and build analytical
models **without relying on cloud infrastructure**, helping reduce cost and
iteration time during development.

---

## ğŸš€ Why this project

In many real-world data projects, running transformations and data quality
checks directly in the cloud can be expensive and slow during early
development.

This project shows how to:

- Run dbt models **locally**
- Perform **data quality validation** before cloud deployment
- Use DuckDB as a lightweight analytical engine
- Organize models using **staging / intermediate / marts** layers

---

## ğŸ› ï¸ Tech Stack

- **Python 3.12**
- **dbt-core**
- **dbt-duckdb**
- **DuckDB**
- **GitHub Actions** (CI)
- **CSV files** as raw data sources

---

## ğŸ“‚ Project Structure

```text
duckdb-main/
â”œâ”€â”€ dbt/
â”‚   â””â”€â”€ customer_orders_analytics/
â”‚       â”œâ”€â”€ data/                 # Raw CSV files
â”‚       â”œâ”€â”€ models/
â”‚       â”‚   â”œâ”€â”€ stg/               # Staging models
â”‚       â”‚   â”œâ”€â”€ int/               # Intermediate models
â”‚       â”‚   â””â”€â”€ marts/             # Final fact tables
â”‚       â”œâ”€â”€ dbt_project.yml
â”‚       â”œâ”€â”€ profiles.yml
â”‚       â””â”€â”€ check_data.py
â”œâ”€â”€ requirements.txt
â””â”€â”€ .github/workflows/             # CI pipeline
